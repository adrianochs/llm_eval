{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Guide to LLM Evals by Aparna Dhinakaran\n",
    "[Link](https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\n",
    "\n",
    "In this tutorial, Aparna shows how to evaluate whether a LLM can class reference texts as relevant or not given a query. I.e. she is evauating RAG relevance. At first read this was not obvious to me from seeing the code so this notebook might be helpful.\n",
    "\n",
    "What is RAG relevance?\n",
    "For example, if I ask an LLM how glaciers are formed, and we provide this LLM with a reference text about glaciers that explains how glacier texts are formed, then this LLM should class the reference text as relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "   RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "   RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "   OpenAIModel,\n",
    "   download_benchmark_dataset,\n",
    "   llm_classify,\n",
    ")\n",
    "import tiktoken\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a \"golden dataset\" built into Phoenix\n",
    "benchmark_dataset = download_benchmark_dataset(\n",
    "   task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a a look at the benchmark dataset\n",
    "benchmark_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so I assume \"query_text\" is the query for your LLM, the \"document_text\" contains the reference text and \"relevant\" is the classification column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = benchmark_dataset.iloc[0,2]\n",
    "document_text = benchmark_dataset.iloc[0,3]\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt to evaluate document reference\n",
    "The library has a template saved that basically instructs the LLM to check if a given text is relevant for the query provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_RELEVANCY_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very simple thing are the \"rails\". It is just a dictionary mapping the binary TRUE or FALSE to relevant and unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_RELEVANCY_PROMPT_RAILS_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG relevance evaluation in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of speed, we'll just sample 100 examples in a repeatable way\n",
    "benchmark_dataset = benchmark_dataset.sample(2, random_state=2023)\n",
    "benchmark_dataset = benchmark_dataset.rename(\n",
    "   columns={\n",
    "       \"query_text\": \"input\",\n",
    "       \"document_text\": \"reference\",\n",
    "   },\n",
    ")\n",
    "# Match the label between our dataset and what the eval will generate\n",
    "y_true = benchmark_dataset[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"})\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any general purpose LLM should work here, but it is best practice to keep the temperature at 0\n",
    "model = OpenAIModel(\n",
    "   model=\"gpt-4\",\n",
    "   temperature=0.0,\n",
    ")\n",
    "\n",
    "# Rails will define our output classes\n",
    "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "\n",
    "y_pred = llm_classify(dataframe=benchmark_dataset, \n",
    "                      model=model,\n",
    "                      template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "                      rails=rails,\n",
    "                      provide_explanation=False)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(RAG_RELEVANCY_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the prompt might change our evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Template_new = '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: {input}\\n    ************\\n    [Reference text]: {reference}\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can answer the Question. Please focus on whether the very specific\\nquestion can be answered by the information in the Reference text.\\nYour response must be single word, either \"relevant\" or \"unrelated\",\\nand should not contain any text or characters aside from that word.\\n\"unrelated\" means that the reference text does not contain an answer to the Question.\\n\"relevant\" means the reference text contains an answer to the Question.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
